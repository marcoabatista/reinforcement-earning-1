{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atividade Prática II - Treinamento e Validação de Modelos de RL\n",
    "\n",
    "**Aluno:** Marco Antonio Batista\n",
    "\n",
    "**Disciplina:** Reinforcement Learning - Turma II\n",
    "\n",
    "**Data:** 21/08/2021\n",
    "\n",
    "\n",
    "\n",
    "Neste trabalho vamos aplicar `Gym`, `Stable-Baselines3` e `RL Baselines Zoo` para lidar com o treinamento e validação de problemas de aprendizado por reforço. Sua tarefa é:\n",
    "\n",
    "1. Selecionar um cenário da biblioteca `Gym` de sua preferência, desde que este cenário também seja contemplado pelos modelos disponibilizados na `rl baselines zoo`;\n",
    "2. Selecionar três algoritmos das biblioteca `Stable-baselines3` para resolver esse problema. Pesquise na documentação da biblioteca quais são os algoritmos mais adequados para o ambiente escolhido e justifique a sua escolha. \n",
    "3. Realize o treinamento de cada um dos três modelos ---você pode ajustar os parâmetros do modelos, se achar necessário--- e salve os modelos em disco.\n",
    "4. De posse dos modelos treinados e salvos, carregue-os e avalie-os por 10 episódios. Apresente os resultados médios e gere a curva de recompensa acumulada disponibilizada pelo `TensorBoard`.\n",
    "5. Compare os resultados dos modelos treinados com os resultados obtidos por modelo(s) existentes no `RL Baselines Zoo` para o cenário escolhido.\n",
    "6. Gere um vídeo do melhor modelo que você treinou e do modelo escolhido na `RL Baselines Zoo`. Verifique a documentação de cada biblioteca sobre a criação do vídeo e visualização em Notebooks.\n",
    "\n",
    "\n",
    "\n",
    "* **Data de entrega:** 04/09/2021\n",
    "* **Local de envio:** AVA.\n",
    "* **Tipo de documento:** Notebook (`.ipynb`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback\n",
    "\n",
    "import time\n",
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('LunarLander-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-01 19:36:25.842035: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-01 19:36:25.842090: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, verbose=0, tensorboard_log=\"./lunar_tensorboard/train_dqn/\")\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "# Save the agent\n",
    "model.save(\"lunar_dqn\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model = PPO('MlpPolicy', env, verbose=0, tensorboard_log=\"./lunar_tensorboard/train_ppo/\")\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "# Save the agent\n",
    "model.save(\"lunar_ppo\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the agent\n",
    "model = A2C('MlpPolicy', env, verbose=0, tensorboard_log=\"./lunar_tensorboard/train_a2c/\")\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100000)\n",
    "# Save the agent\n",
    "model.save(\"lunar_a2c\")\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisar os dados de treinamento no TensorBoard\n",
    "#!tensorboard --logdir ./lunar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:-96.67 +/- 46.50\n"
     ]
    }
   ],
   "source": [
    "# Load the trained agent\n",
    "model = DQN.load(\"lunar_dqn\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(3000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.0003)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    " \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = PPO.load(\"lunar_ppo\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(3000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.0003)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "    \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = A2C.load(\"lunar_a2c\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(3000):\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    time.sleep(0.0003)\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "    \n",
    "env.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marco/rf/rl-baselines3-zoo\n"
     ]
    }
   ],
   "source": [
    "#!git clone --recursive https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "%cd rl-baselines3-zoo/\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LunarLander-v2 ==========\n",
      "Seed: 287680758\n",
      "Log path: logs/dqn/LunarLander-v2_4\n",
      "2021-09-01 19:46:57.891537: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-01 19:46:57.891575: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Eval num_timesteps=10000, episode_reward=-94.04 +/- 40.17\n",
      "Episode length: 677.60 +/- 396.07\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=42.89 +/- 149.97\n",
      "Episode length: 278.00 +/- 182.54\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=165.46 +/- 99.77\n",
      "Episode length: 597.80 +/- 247.05\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=152.68 +/- 94.75\n",
      "Episode length: 586.20 +/- 348.68\n",
      "Eval num_timesteps=50000, episode_reward=260.36 +/- 12.92\n",
      "Episode length: 326.80 +/- 51.59\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=165.61 +/- 103.00\n",
      "Episode length: 622.40 +/- 312.96\n",
      "Eval num_timesteps=70000, episode_reward=207.34 +/- 111.13\n",
      "Episode length: 436.60 +/- 306.52\n",
      "Eval num_timesteps=80000, episode_reward=-4.77 +/- 183.21\n",
      "Episode length: 565.80 +/- 278.64\n",
      "Eval num_timesteps=90000, episode_reward=100.75 +/- 127.07\n",
      "Episode length: 551.20 +/- 373.52\n",
      "Eval num_timesteps=100000, episode_reward=282.97 +/- 19.98\n",
      "Episode length: 195.80 +/- 24.80\n",
      "New best mean reward!\n",
      "Saving to logs/dqn/LunarLander-v2_4\n"
     ]
    }
   ],
   "source": [
    "#RL Baselines3 Zoo - avaliação via TensorBoard DQN\n",
    "!python train.py --algo dqn --env LunarLander-v2 --verbose 0 --tensorboard-log ../lunar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LunarLander-v2 ==========\n",
      "Seed: 3698498684\n",
      "Log path: logs/ppo/LunarLander-v2_4\n",
      "2021-09-01 19:54:36.165229: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-01 19:54:36.165265: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Eval num_timesteps=10000, episode_reward=-148.90 +/- 61.77\n",
      "Episode length: 71.20 +/- 12.06\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-161.41 +/- 61.85\n",
      "Episode length: 98.20 +/- 39.97\n",
      "Eval num_timesteps=30000, episode_reward=-193.31 +/- 132.50\n",
      "Episode length: 110.20 +/- 42.34\n",
      "Eval num_timesteps=40000, episode_reward=-778.93 +/- 473.02\n",
      "Episode length: 221.40 +/- 109.70\n",
      "Eval num_timesteps=50000, episode_reward=-155.99 +/- 227.62\n",
      "Episode length: 414.80 +/- 294.00\n",
      "Eval num_timesteps=60000, episode_reward=-215.00 +/- 158.19\n",
      "Episode length: 472.60 +/- 345.63\n",
      "Eval num_timesteps=70000, episode_reward=-26.40 +/- 41.50\n",
      "Episode length: 121.80 +/- 46.43\n",
      "New best mean reward!\n",
      "Eval num_timesteps=80000, episode_reward=-35.60 +/- 157.95\n",
      "Episode length: 178.00 +/- 87.58\n",
      "Eval num_timesteps=90000, episode_reward=-136.30 +/- 91.48\n",
      "Episode length: 331.00 +/- 154.55\n",
      "Eval num_timesteps=100000, episode_reward=-38.90 +/- 92.99\n",
      "Episode length: 518.20 +/- 256.38\n",
      "Eval num_timesteps=110000, episode_reward=34.76 +/- 103.67\n",
      "Episode length: 548.20 +/- 249.65\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-72.06 +/- 95.29\n",
      "Episode length: 786.60 +/- 177.18\n",
      "Eval num_timesteps=130000, episode_reward=-16.68 +/- 104.29\n",
      "Episode length: 725.20 +/- 214.75\n",
      "Eval num_timesteps=140000, episode_reward=-156.72 +/- 45.94\n",
      "Episode length: 602.20 +/- 156.80\n",
      "Eval num_timesteps=150000, episode_reward=-113.38 +/- 23.98\n",
      "Episode length: 335.00 +/- 23.92\n",
      "Eval num_timesteps=160000, episode_reward=-178.64 +/- 67.35\n",
      "Episode length: 778.80 +/- 122.54\n",
      "Eval num_timesteps=170000, episode_reward=-89.12 +/- 29.23\n",
      "Episode length: 773.40 +/- 285.29\n",
      "Eval num_timesteps=180000, episode_reward=-162.76 +/- 54.08\n",
      "Episode length: 776.80 +/- 247.18\n",
      "Eval num_timesteps=190000, episode_reward=-160.77 +/- 79.01\n",
      "Episode length: 850.80 +/- 195.72\n",
      "Eval num_timesteps=200000, episode_reward=-190.40 +/- 69.87\n",
      "Episode length: 757.60 +/- 236.06\n",
      "Eval num_timesteps=210000, episode_reward=-181.39 +/- 62.92\n",
      "Episode length: 621.40 +/- 181.40\n",
      "Eval num_timesteps=220000, episode_reward=-108.28 +/- 74.83\n",
      "Episode length: 979.80 +/- 40.40\n",
      "Eval num_timesteps=230000, episode_reward=-171.66 +/- 56.99\n",
      "Episode length: 875.20 +/- 182.27\n",
      "Eval num_timesteps=240000, episode_reward=-179.06 +/- 71.17\n",
      "Episode length: 801.20 +/- 211.09\n",
      "Eval num_timesteps=250000, episode_reward=-86.71 +/- 66.49\n",
      "Episode length: 929.00 +/- 142.00\n",
      "Eval num_timesteps=260000, episode_reward=-128.10 +/- 60.67\n",
      "Episode length: 984.00 +/- 32.00\n",
      "Eval num_timesteps=270000, episode_reward=-66.07 +/- 6.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=280000, episode_reward=-57.24 +/- 18.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=290000, episode_reward=-70.84 +/- 32.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=300000, episode_reward=-49.92 +/- 16.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=310000, episode_reward=-32.45 +/- 23.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "Eval num_timesteps=320000, episode_reward=-51.06 +/- 32.06\n",
      "Episode length: 798.80 +/- 253.05\n",
      "Eval num_timesteps=330000, episode_reward=114.16 +/- 95.78\n",
      "Episode length: 730.20 +/- 135.44\n",
      "New best mean reward!\n",
      "Eval num_timesteps=340000, episode_reward=50.39 +/- 111.83\n",
      "Episode length: 685.80 +/- 96.73\n",
      "Eval num_timesteps=350000, episode_reward=65.35 +/- 112.00\n",
      "Episode length: 612.40 +/- 190.46\n",
      "Eval num_timesteps=360000, episode_reward=56.84 +/- 145.13\n",
      "Episode length: 709.40 +/- 97.75\n",
      "Eval num_timesteps=370000, episode_reward=73.62 +/- 94.89\n",
      "Episode length: 727.20 +/- 145.80\n",
      "Eval num_timesteps=380000, episode_reward=83.80 +/- 106.03\n",
      "Episode length: 744.20 +/- 134.74\n",
      "Eval num_timesteps=390000, episode_reward=130.20 +/- 100.03\n",
      "Episode length: 693.00 +/- 45.17\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=107.99 +/- 70.78\n",
      "Episode length: 723.80 +/- 112.61\n",
      "Eval num_timesteps=410000, episode_reward=155.87 +/- 72.00\n",
      "Episode length: 698.60 +/- 153.88\n",
      "New best mean reward!\n",
      "Eval num_timesteps=420000, episode_reward=201.90 +/- 22.89\n",
      "Episode length: 621.40 +/- 48.86\n",
      "New best mean reward!\n",
      "Eval num_timesteps=430000, episode_reward=205.93 +/- 16.67\n",
      "Episode length: 516.20 +/- 31.76\n",
      "New best mean reward!\n",
      "Eval num_timesteps=440000, episode_reward=138.13 +/- 103.67\n",
      "Episode length: 607.80 +/- 108.14\n",
      "Eval num_timesteps=450000, episode_reward=204.48 +/- 21.27\n",
      "Episode length: 476.60 +/- 17.68\n",
      "Eval num_timesteps=460000, episode_reward=185.98 +/- 34.46\n",
      "Episode length: 552.80 +/- 53.00\n",
      "Eval num_timesteps=470000, episode_reward=61.72 +/- 111.58\n",
      "Episode length: 544.20 +/- 70.73\n",
      "Eval num_timesteps=480000, episode_reward=153.72 +/- 55.98\n",
      "Episode length: 722.80 +/- 226.85\n",
      "Eval num_timesteps=490000, episode_reward=142.91 +/- 48.80\n",
      "Episode length: 712.60 +/- 234.68\n",
      "Eval num_timesteps=500000, episode_reward=83.89 +/- 79.10\n",
      "Episode length: 703.60 +/- 244.20\n",
      "Eval num_timesteps=510000, episode_reward=70.62 +/- 126.25\n",
      "Episode length: 593.60 +/- 203.99\n",
      "Eval num_timesteps=520000, episode_reward=104.56 +/- 128.96\n",
      "Episode length: 490.20 +/- 23.63\n",
      "Eval num_timesteps=530000, episode_reward=157.16 +/- 86.40\n",
      "Episode length: 518.60 +/- 22.03\n",
      "Eval num_timesteps=540000, episode_reward=138.84 +/- 96.10\n",
      "Episode length: 504.20 +/- 40.86\n",
      "Eval num_timesteps=550000, episode_reward=211.95 +/- 14.56\n",
      "Episode length: 495.20 +/- 9.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=560000, episode_reward=202.92 +/- 57.80\n",
      "Episode length: 596.60 +/- 202.08\n",
      "Eval num_timesteps=570000, episode_reward=83.89 +/- 122.69\n",
      "Episode length: 571.40 +/- 218.48\n",
      "Eval num_timesteps=580000, episode_reward=214.74 +/- 48.59\n",
      "Episode length: 576.60 +/- 212.24\n",
      "New best mean reward!\n",
      "Eval num_timesteps=590000, episode_reward=188.33 +/- 64.84\n",
      "Episode length: 664.60 +/- 227.62\n",
      "Eval num_timesteps=600000, episode_reward=173.44 +/- 67.11\n",
      "Episode length: 683.20 +/- 259.19\n",
      "Eval num_timesteps=610000, episode_reward=228.00 +/- 18.63\n",
      "Episode length: 448.00 +/- 29.22\n",
      "New best mean reward!\n",
      "Eval num_timesteps=620000, episode_reward=149.91 +/- 60.86\n",
      "Episode length: 785.20 +/- 263.15\n",
      "Eval num_timesteps=630000, episode_reward=240.71 +/- 21.81\n",
      "Episode length: 499.60 +/- 175.26\n",
      "New best mean reward!\n",
      "Eval num_timesteps=640000, episode_reward=218.49 +/- 55.16\n",
      "Episode length: 518.00 +/- 241.12\n",
      "Eval num_timesteps=650000, episode_reward=175.99 +/- 98.55\n",
      "Episode length: 377.20 +/- 40.79\n",
      "Eval num_timesteps=660000, episode_reward=215.60 +/- 68.32\n",
      "Episode length: 521.20 +/- 239.60\n",
      "Eval num_timesteps=670000, episode_reward=244.92 +/- 14.40\n",
      "Episode length: 388.40 +/- 9.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=680000, episode_reward=258.82 +/- 21.26\n",
      "Episode length: 419.60 +/- 9.69\n",
      "New best mean reward!\n",
      "Eval num_timesteps=690000, episode_reward=240.15 +/- 22.57\n",
      "Episode length: 402.80 +/- 10.81\n",
      "Eval num_timesteps=700000, episode_reward=246.23 +/- 20.74\n",
      "Episode length: 407.00 +/- 20.82\n",
      "Eval num_timesteps=710000, episode_reward=248.30 +/- 16.13\n",
      "Episode length: 418.20 +/- 13.00\n",
      "Eval num_timesteps=720000, episode_reward=253.27 +/- 8.16\n",
      "Episode length: 406.40 +/- 4.32\n",
      "Eval num_timesteps=730000, episode_reward=231.53 +/- 20.58\n",
      "Episode length: 396.20 +/- 16.94\n",
      "Eval num_timesteps=740000, episode_reward=237.26 +/- 14.50\n",
      "Episode length: 379.60 +/- 16.52\n",
      "Eval num_timesteps=750000, episode_reward=251.66 +/- 21.02\n",
      "Episode length: 400.80 +/- 15.77\n",
      "Eval num_timesteps=760000, episode_reward=221.12 +/- 14.87\n",
      "Episode length: 370.40 +/- 2.94\n",
      "Eval num_timesteps=770000, episode_reward=203.24 +/- 54.79\n",
      "Episode length: 496.20 +/- 252.15\n",
      "Eval num_timesteps=780000, episode_reward=218.18 +/- 94.71\n",
      "Episode length: 335.60 +/- 33.36\n",
      "Eval num_timesteps=790000, episode_reward=241.40 +/- 26.19\n",
      "Episode length: 360.40 +/- 13.94\n",
      "Eval num_timesteps=800000, episode_reward=257.99 +/- 16.88\n",
      "Episode length: 357.20 +/- 20.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=810000, episode_reward=248.62 +/- 17.09\n",
      "Episode length: 355.60 +/- 23.10\n",
      "Eval num_timesteps=820000, episode_reward=239.38 +/- 17.61\n",
      "Episode length: 356.20 +/- 16.71\n",
      "Eval num_timesteps=830000, episode_reward=251.25 +/- 22.85\n",
      "Episode length: 352.80 +/- 10.13\n",
      "Eval num_timesteps=840000, episode_reward=254.02 +/- 22.25\n",
      "Episode length: 373.40 +/- 15.45\n",
      "Eval num_timesteps=850000, episode_reward=231.16 +/- 18.78\n",
      "Episode length: 358.20 +/- 12.92\n",
      "Eval num_timesteps=860000, episode_reward=248.52 +/- 12.74\n",
      "Episode length: 393.00 +/- 16.25\n",
      "Eval num_timesteps=870000, episode_reward=240.40 +/- 6.17\n",
      "Episode length: 411.60 +/- 18.38\n",
      "Eval num_timesteps=880000, episode_reward=237.59 +/- 21.37\n",
      "Episode length: 396.20 +/- 15.03\n",
      "Eval num_timesteps=890000, episode_reward=230.89 +/- 14.87\n",
      "Episode length: 405.80 +/- 18.13\n",
      "Eval num_timesteps=900000, episode_reward=237.77 +/- 29.19\n",
      "Episode length: 421.60 +/- 13.47\n",
      "Eval num_timesteps=910000, episode_reward=248.48 +/- 22.40\n",
      "Episode length: 399.40 +/- 32.96\n",
      "Eval num_timesteps=920000, episode_reward=220.82 +/- 55.68\n",
      "Episode length: 488.40 +/- 256.78\n",
      "Eval num_timesteps=930000, episode_reward=260.99 +/- 19.13\n",
      "Episode length: 364.20 +/- 18.63\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=252.91 +/- 21.54\n",
      "Episode length: 368.80 +/- 7.52\n",
      "Eval num_timesteps=950000, episode_reward=254.87 +/- 15.85\n",
      "Episode length: 347.80 +/- 15.59\n",
      "Eval num_timesteps=960000, episode_reward=232.81 +/- 9.72\n",
      "Episode length: 358.40 +/- 9.85\n",
      "Eval num_timesteps=970000, episode_reward=244.35 +/- 14.58\n",
      "Episode length: 340.40 +/- 10.37\n",
      "Eval num_timesteps=980000, episode_reward=243.00 +/- 24.85\n",
      "Episode length: 334.20 +/- 12.94\n",
      "Eval num_timesteps=990000, episode_reward=237.57 +/- 22.88\n",
      "Episode length: 337.80 +/- 11.18\n",
      "Eval num_timesteps=1000000, episode_reward=247.57 +/- 22.94\n",
      "Episode length: 320.60 +/- 9.95\n",
      "Eval num_timesteps=1010000, episode_reward=265.84 +/- 29.56\n",
      "Episode length: 330.40 +/- 17.76\n",
      "New best mean reward!\n",
      "Saving to logs/ppo/LunarLander-v2_4\n"
     ]
    }
   ],
   "source": [
    "#RL Baselines3 Zoo - avaliação via TensorBoard PPO\n",
    "!python train.py --algo ppo --env LunarLander-v2 --verbose 0 --tensorboard-log ../lunar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== LunarLander-v2 ==========\n",
      "Seed: 599132\n",
      "Log path: logs/a2c/LunarLander-v2_5\n",
      "2021-09-01 20:20:03.431910: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-01 20:20:03.432005: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Eval num_timesteps=10000, episode_reward=-4159.30 +/- 1010.45\n",
      "Episode length: 844.40 +/- 82.42\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=-1588.27 +/- 85.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=30000, episode_reward=-1255.24 +/- 112.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=-148.62 +/- 247.28\n",
      "Episode length: 917.40 +/- 125.53\n",
      "New best mean reward!\n",
      "Eval num_timesteps=50000, episode_reward=-64.51 +/- 173.73\n",
      "Episode length: 634.00 +/- 196.41\n",
      "New best mean reward!\n",
      "Eval num_timesteps=60000, episode_reward=-454.14 +/- 258.94\n",
      "Episode length: 824.40 +/- 283.93\n",
      "Eval num_timesteps=70000, episode_reward=-252.48 +/- 219.83\n",
      "Episode length: 682.00 +/- 291.91\n",
      "Eval num_timesteps=80000, episode_reward=-267.84 +/- 262.11\n",
      "Episode length: 756.60 +/- 172.28\n",
      "Eval num_timesteps=90000, episode_reward=-27.68 +/- 143.92\n",
      "Episode length: 507.80 +/- 262.91\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=-115.33 +/- 214.47\n",
      "Episode length: 826.80 +/- 288.00\n",
      "Eval num_timesteps=110000, episode_reward=1.49 +/- 191.46\n",
      "Episode length: 627.00 +/- 309.84\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=-111.22 +/- 81.32\n",
      "Episode length: 774.80 +/- 280.76\n",
      "Eval num_timesteps=130000, episode_reward=7.89 +/- 157.94\n",
      "Episode length: 508.00 +/- 404.27\n",
      "New best mean reward!\n",
      "Eval num_timesteps=140000, episode_reward=-44.43 +/- 36.55\n",
      "Episode length: 670.00 +/- 407.09\n",
      "Eval num_timesteps=150000, episode_reward=-21.39 +/- 132.68\n",
      "Episode length: 719.60 +/- 358.02\n",
      "Eval num_timesteps=160000, episode_reward=61.59 +/- 135.71\n",
      "Episode length: 311.00 +/- 163.13\n",
      "New best mean reward!\n",
      "Eval num_timesteps=170000, episode_reward=83.17 +/- 149.40\n",
      "Episode length: 434.40 +/- 315.56\n",
      "New best mean reward!\n",
      "Eval num_timesteps=180000, episode_reward=-60.75 +/- 55.27\n",
      "Episode length: 654.80 +/- 424.21\n",
      "Eval num_timesteps=190000, episode_reward=-67.02 +/- 42.29\n",
      "Episode length: 682.00 +/- 391.02\n",
      "Eval num_timesteps=200000, episode_reward=-9.69 +/- 163.58\n",
      "Episode length: 686.00 +/- 385.82\n",
      "Saving to logs/a2c/LunarLander-v2_5\n"
     ]
    }
   ],
   "source": [
    "#RL Baselines3 Zoo - avaliação via TensorBoard A2C\n",
    "!python train.py --algo a2c --env LunarLander-v2 --verbose 0 --tensorboard-log ../lunar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/marco/rf\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analisar os dados de treinamento e do RL Baselines3 Zoo no TensorBoard\n",
    "#!tensorboard --logdir ./lunar_tensorboard/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"lunar_ppo\", env=env)\n",
    "\n",
    "images = []\n",
    "obs = env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "for i in range(1300):\n",
    "    images.append(img)\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    img = env.render(mode='rgb_array')\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "imageio.mimsave('lander_ppo.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"lunar_a2c\", env=env)\n",
    "\n",
    "images = []\n",
    "obs = env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "for i in range(1300):\n",
    "    images.append(img)\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    img = env.render(mode='rgb_array')\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "imageio.mimsave('lander_a2c.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN.load(\"lunar_dqn\", env=env)\n",
    "\n",
    "images = []\n",
    "obs = env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "for i in range(1300):\n",
    "    images.append(img)\n",
    "    action, _state = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    img = env.render(mode='rgb_array')\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "\n",
    "imageio.mimsave('lander_dqn.gif', [np.array(img) for i, img in enumerate(images) if i%2 == 0], fps=29)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'rl-baselines3-zoo/'\n",
      "/home/marco/rf/rl-baselines3-zoo\n",
      "Loading latest experiment, id=4\n",
      "Saving video to /home/marco/rf/rl-baselines3-zoo/logs/ppo/LunarLander-v2_4/videos/final-model-ppo-LunarLander-v2-step-0-to-step-1000.mp4\n",
      "Exception ignored in: <function VecVideoRecorder.__del__ at 0x7f520d5e11f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marco/rf/rf/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 113, in __del__\n",
      "  File \"/home/marco/rf/rf/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 109, in close\n",
      "AttributeError: 'NoneType' object has no attribute 'close'\n",
      "Saving video to /home/marco/rf/rl-baselines3-zoo/logs/ppo/LunarLander-v2_4/videos/best-model-ppo-LunarLander-v2-step-0-to-step-1000.mp4\n",
      "Exception ignored in: <function VecVideoRecorder.__del__ at 0x7fb9ed19b1f0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/marco/rf/rf/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 113, in __del__\n",
      "  File \"/home/marco/rf/rf/lib/python3.8/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py\", line 109, in close\n",
      "AttributeError: 'NoneType' object has no attribute 'close'\n",
      "Saving video to /home/marco/rf/rl-baselines3-zoo/logs/ppo/LunarLander-v2_4/videos/training.mp4\n",
      "Saving gif to /home/marco/rf/rl-baselines3-zoo/logs/ppo/LunarLander-v2_4/videos/training.gif\n"
     ]
    }
   ],
   "source": [
    "%cd rl-baselines3-zoo/\n",
    "#!python -m utils.record_video --algo ppo --env LunarLander-v2 -n 1000\n",
    "!python -m utils.record_training --algo ppo --env LunarLander-v2 -n 1000 -f logs --deterministic --gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
